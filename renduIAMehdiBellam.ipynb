{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer Un dataset préfait (MNIST)\n",
    "\n",
    "training_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset)\n",
    "print(training_dataset.data.shape)\n",
    "print(training_dataset.targets.shape)\n",
    "print(training_dataset.targets.unique())\n",
    "print(training_dataset.targets.bincount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_dataset), size=(1,)).item()\n",
    "    img, label = training_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape X : torch.Size([64, 1, 28, 28])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"shape X : {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fcs = nn.ModuleList(\n",
    "            [nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)]\n",
    "        )\n",
    "        self.act = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) #1 * 28 * 28 -> 1 * 784 \n",
    "        for layer in self.fcs:\n",
    "            x  = layer(x)\n",
    "            x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "output_dim = 10 #Nombre de classe\n",
    "model = MLP([input_dim, 128, 128, output_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fcs): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (act): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # Expect raw logits (!= probabilities)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        #Prédiction\n",
    "        ypred = model(X)\n",
    "        loss = loss_fn(ypred, y) #Calcul de l'erreur\n",
    "\n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step() #W = W - lr * grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size * len(X)\n",
    "            print(f'loss {loss:>7f} [{current:>5d}/{len(dataloader.dataset)}]')\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            ypred = model(X)\n",
    "            test_loss += loss_fn(ypred, y).item()\n",
    "            correct += (ypred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss/= len(dataloader)\n",
    "    correct/=len(dataloader.dataset)\n",
    "    print(f\"Test loss: {test_loss:>8f} | test accuracy {(correct * 100):>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1} ------------------------\")\n",
    "#     train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "#     test_loop(test_dataloader, model, loss_fn)\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer Un dataset préfait (MNIST)\n",
    "\n",
    "training_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.13092496991157532\n",
      "Vriance: 0.30183959007263184\n"
     ]
    }
   ],
   "source": [
    "# moyenne et variance des images afin que le model voit mieux en divisant par la moyenne et la variance\n",
    "from torch.utils.data import ConcatDataset\n",
    "mean , std = 0 , 0\n",
    "combined_dataset = ConcatDataset([training_dataset, test_dataset])\n",
    "\n",
    "for image , label in combined_dataset:\n",
    "    image = image.view(28*28)\n",
    "    mean += image.mean().sum()\n",
    "    std += image.std().sum()\n",
    "\n",
    "mean /= len(combined_dataset)\n",
    "std /= len(combined_dataset)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Vriance: {std}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer Un dataset préfait (MNIST)\n",
    "\n",
    "training_datasetV2 = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.Compose([T.ToTensor() , T.Normalize((mean,), (std))])\n",
    ")\n",
    "\n",
    "test_datasetV2 = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=T.Compose([T.ToTensor() , T.Normalize((mean,), (std))])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1.2527321385667684e-06\n",
      "Variance: 1.0000014643941606\n"
     ]
    }
   ],
   "source": [
    "# moyenne et variance des images afin que le model voit mieux en divisant par la moyenne et la variance\n",
    "from torch.utils.data import ConcatDataset\n",
    "mean, std = 0, 0\n",
    "combined_dataset = ConcatDataset([training_datasetV2, test_datasetV2])\n",
    "\n",
    "for image, label in combined_dataset:\n",
    "    image = image.view(28*28)\n",
    "    mean += image.mean().item()\n",
    "    std += image.std().item()\n",
    "\n",
    "mean /= len(combined_dataset)\n",
    "std /= len(combined_dataset)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Variance: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.3948, -0.1999,\n",
       "           -0.1999, -0.1999,  1.2033,  1.3332,  1.8399, -0.0960,  1.7230,\n",
       "            2.8793,  2.7753,  1.2163, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.0440,  0.0340,  0.7875,  1.5670,  1.7749,  2.8533,\n",
       "            2.8533,  2.8533,  2.8533,  2.8533,  2.4895,  1.8009,  2.8533,\n",
       "            2.7104,  2.0997,  0.3977, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "            0.2029,  2.6584,  2.8533,  2.8533,  2.8533,  2.8533,  2.8533,\n",
       "            2.8533,  2.8533,  2.8533,  2.8273,  0.7745,  0.6316,  0.6316,\n",
       "            0.2938,  0.0729, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.1999,  2.4115,  2.8533,  2.8533,  2.8533,  2.8533,  2.8533,\n",
       "            2.1387,  1.9308,  2.7753,  2.6974, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338,  0.6056,  1.5930,  0.9564,  2.8533,  2.8533,  2.2297,\n",
       "           -0.2908, -0.4338,  0.1249,  1.5670, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.2519, -0.4208,  1.5670,  2.8533,  0.7355,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338,  1.3722,  2.8533,  2.0348,\n",
       "           -0.4078, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.2908,  2.0348,  2.8533,\n",
       "            0.4757, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,  0.0210,  2.6974,\n",
       "            2.4895,  1.6450,  0.9694, -0.4208, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,  0.6186,\n",
       "            2.6844,  2.8533,  2.8533,  1.1123, -0.1090, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "            0.1509,  1.9828,  2.8533,  2.8533,  1.5151, -0.0830, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.2259,  0.7745,  2.8403,  2.8533,  1.9958, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338,  2.8013,  2.8533,  2.8013,  0.3977,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "            0.1639,  1.2552,  1.9438,  2.8533,  2.8533,  2.2556, -0.4078,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,  0.0729,  1.4891,\n",
       "            2.5415,  2.8533,  2.8533,  2.8533,  2.8143,  1.9308, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.1219,  1.0474,  2.4375,  2.8533,\n",
       "            2.8533,  2.8533,  2.8533,  2.1777,  0.5796, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.1349,  0.4237,  2.3336,  2.8533,  2.8533,  2.8533,\n",
       "            2.8533,  2.1387,  0.6186, -0.4078, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.1999,\n",
       "            1.7879,  2.4115,  2.8533,  2.8533,  2.8533,  2.8533,  2.0997,\n",
       "            0.6056, -0.3168, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338,  0.2808,  1.8009,  2.5025,\n",
       "            2.8533,  2.8533,  2.8533,  2.8533,  2.7363,  1.2942, -0.2908,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338,  1.3332,  2.8533,  2.8533,\n",
       "            2.8533,  2.3206,  1.3202,  1.2812, -0.2259, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_datasetV2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1} ------------------------\")\n",
    "#     train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "#     test_loop(test_dataloader, model, loss_fn)\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerateur cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fcs): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (act): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"accelerateur {device}\")\n",
    "\n",
    "input_dim = 28*28\n",
    "output_dim = 10 #Nombre de classe\n",
    "model = MLP([input_dim, 128, 128, output_dim]).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device) # Expect raw logits (!= probabilities)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tensorboard package\n",
    "# %pip install tensorboard\n",
    "# tensorboard --logdir=runs\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/mnist_experiment_1')\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    return epoch_loss / len(dataloader), correct / len(dataloader.dataset) * 100\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    return test_loss / len(dataloader), correct / len(dataloader.dataset) * 100\n",
    "\n",
    "# Training loop with added graphing\n",
    "def train_and_test(dataloader_train, dataloader_test, model, loss_fn, optimizer, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Train the model\n",
    "        train_loss, accuracy = train_loop(dataloader_train, model, loss_fn, optimizer, batch_size)\n",
    "        writer.add_scalars('Loss', {'train': train_loss}, epoch)\n",
    "        writer.add_scalars('Accuracy', {'train': accuracy}, epoch)\n",
    "\n",
    "        # Test the model\n",
    "        test_loss, accuracy = test_loop(dataloader_test, model, loss_fn)\n",
    "        writer.add_scalars('Loss', {'test': test_loss}, epoch)\n",
    "        writer.add_scalars('Accuracy', {'test': accuracy}, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} complete\\n\")\n",
    "\n",
    "#train_and_test(train_dataloader, test_dataloader, model, loss_fn, optimizer, epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# !tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1), #1 * 28 * 28 -> #32 * 28 * 28\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1), #32 * 28 * 28 -> 64 * 28 * 28\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) #64 * 28 *28 -> 64 * 14 * 14\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 14 * 14, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 1/10 complete\n",
      "\n",
      "Epoch 2/10\n",
      "Epoch 2/10 complete\n",
      "\n",
      "Epoch 3/10\n",
      "Epoch 3/10 complete\n",
      "\n",
      "Epoch 4/10\n",
      "Epoch 4/10 complete\n",
      "\n",
      "Epoch 5/10\n",
      "Epoch 5/10 complete\n",
      "\n",
      "Epoch 6/10\n",
      "Epoch 6/10 complete\n",
      "\n",
      "Epoch 7/10\n",
      "Epoch 7/10 complete\n",
      "\n",
      "Epoch 8/10\n",
      "Epoch 8/10 complete\n",
      "\n",
      "Epoch 9/10\n",
      "Epoch 9/10 complete\n",
      "\n",
      "Epoch 10/10\n",
      "Epoch 10/10 complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialisation du model\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "cnn = CNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device) # Expect raw logits (!= probabilities)\n",
    "optimizer = torch.optim.AdamW(cnn.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "writter = SummaryWriter('runs/mnist_experiment_2')\n",
    "train_and_test(train_dataloader, test_dataloader, cnn, loss_fn, optimizer, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.to(\"cpu\")\n",
    "torch_input = torch.randn(1, 1, 28, 28) #1 image, 1 channel, 28 * 28 \n",
    "onn_program = torch.onnx.export(\n",
    "    cnn,\n",
    "    torch_input,\n",
    "    \"ModelMehdi.onnx\",\n",
    "    verbose=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=11, #  !! version\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 1\n",
      "Sample input: [tensor([[[[ 4.3022e-01, -1.9820e-01, -4.8226e-01,  5.9292e-02, -6.3567e-01,\n",
      "           -1.4522e+00,  4.4610e-01, -1.4602e-01, -3.1005e-01,  5.8623e-01,\n",
      "           -2.7649e-01,  1.0564e+00, -6.2817e-01,  2.0320e+00, -7.3200e-01,\n",
      "           -1.1162e+00,  6.0374e-03, -1.6962e-01,  7.6106e-01, -9.7956e-01,\n",
      "           -5.7656e-01, -4.0037e-01, -1.1802e+00, -1.2862e-01, -6.6312e-01,\n",
      "           -1.2260e+00,  1.1501e+00,  7.9426e-01],\n",
      "          [ 1.2902e+00,  1.2930e+00, -8.7003e-01, -1.5766e+00, -1.4813e+00,\n",
      "           -1.4893e+00, -7.3307e-01,  2.8198e+00, -1.0460e+00, -6.1777e-01,\n",
      "            7.7839e-01, -4.4977e-01,  1.1810e+00,  1.1201e+00, -7.2267e-01,\n",
      "           -6.5290e-01,  7.7540e-01, -9.2666e-01, -3.8030e-01, -1.1868e+00,\n",
      "            5.6334e-01,  7.0307e-01, -2.5499e+00, -2.0507e-01,  1.1439e+00,\n",
      "           -2.8508e-01,  1.3555e+00,  1.6358e+00],\n",
      "          [ 9.6371e-01, -5.0703e-01, -7.5683e-01,  1.0987e+00,  5.7589e-01,\n",
      "           -1.9137e+00, -4.7414e-01,  1.0861e+00, -1.1956e+00, -9.4451e-01,\n",
      "            8.5213e-01,  1.4025e+00, -8.2938e-01, -1.6252e+00,  1.1506e-01,\n",
      "            2.3412e-01,  1.5358e+00,  7.4723e-02,  5.5312e-02,  7.8010e-02,\n",
      "           -1.8503e+00, -3.2106e-01, -8.2782e-01, -2.5594e-02, -1.6409e-01,\n",
      "           -6.8368e-01,  9.7643e-01,  1.8121e-01],\n",
      "          [-1.1272e+00,  6.9609e-01,  1.5357e-01, -2.8255e+00,  2.9255e-01,\n",
      "            2.2382e-01, -1.5095e+00, -1.5653e-01,  2.2701e+00, -1.7052e-01,\n",
      "           -1.3182e+00, -1.3285e+00,  1.0149e+00,  4.0596e-01,  4.9934e-01,\n",
      "            1.7850e+00, -9.4567e-02, -6.7437e-01, -5.1826e-01, -6.7093e-01,\n",
      "           -3.3780e-01, -7.0739e-01,  1.3131e+00,  1.5834e+00,  1.9735e-01,\n",
      "           -1.1704e+00,  1.0404e-01,  9.2053e-01],\n",
      "          [ 9.9446e-01, -1.2422e+00,  1.7509e+00,  4.5516e-01,  1.1033e+00,\n",
      "           -7.6755e-01, -1.1795e+00,  2.4563e-01,  1.1350e+00, -2.4287e-02,\n",
      "           -5.0227e-01, -9.2132e-01, -3.5407e-01,  2.8980e-01, -3.9304e-01,\n",
      "            1.0782e+00,  7.5527e-01, -7.4241e-01,  8.9793e-01,  7.9271e-01,\n",
      "            5.9193e-01,  4.5301e-01,  1.2845e+00, -6.0120e-01, -1.9376e+00,\n",
      "            7.3000e-01,  6.2704e-01,  1.4868e+00],\n",
      "          [-9.9342e-01, -1.2769e+00,  2.7979e-01,  1.4233e-01,  2.0672e-02,\n",
      "           -9.9856e-01,  1.1312e-01, -2.3313e+00, -1.3458e+00, -1.4022e+00,\n",
      "            1.3542e+00,  3.9009e-01, -8.8150e-02,  1.5302e-01,  4.0255e-01,\n",
      "            4.2693e-01,  1.4930e+00,  1.0005e+00, -1.0865e+00,  2.9962e-02,\n",
      "            1.9428e+00,  1.3981e+00, -1.6735e+00, -1.5027e+00, -4.2715e-02,\n",
      "           -1.7859e-02,  2.3291e-01,  1.3661e+00],\n",
      "          [ 2.1501e-01,  1.7148e+00, -8.0052e-01,  4.8555e-01, -2.2879e+00,\n",
      "            8.2127e-01, -1.3816e-01, -6.3907e-01, -4.8512e-02, -2.5731e+00,\n",
      "            1.0596e+00, -6.1214e-01, -1.2209e+00, -4.3861e-01,  2.6035e-01,\n",
      "            2.5827e-01, -8.2810e-01,  1.3690e-02, -1.0872e+00,  7.7334e-01,\n",
      "           -4.0740e-01, -9.9850e-01,  2.0276e+00, -1.8036e+00, -1.2576e+00,\n",
      "           -8.8346e-01, -1.4146e-01, -2.1877e+00],\n",
      "          [ 4.5422e-01, -7.5633e-01,  5.0892e-01,  4.3905e-01, -1.4992e+00,\n",
      "           -3.1891e+00, -1.2767e+00, -7.5920e-01, -8.5546e-01,  1.6214e+00,\n",
      "            2.0625e-01,  6.8886e-01, -2.1526e+00, -6.1203e-02, -1.3142e+00,\n",
      "           -1.8465e+00, -6.3522e-01,  1.6071e-01,  2.2435e-01,  6.6979e-01,\n",
      "            1.1410e+00, -8.6023e-01, -5.5638e-01, -6.5968e-01,  5.7570e-01,\n",
      "            1.1837e+00,  3.6418e-01, -2.0254e-02],\n",
      "          [ 4.1432e-01, -2.2552e+00, -3.9597e-01, -4.3353e-01, -1.7755e+00,\n",
      "           -1.5001e+00, -6.3591e-02,  1.4970e+00,  3.7742e-01,  4.9125e-01,\n",
      "           -1.4750e-01, -1.3015e-01, -1.4590e-01,  8.9515e-02,  6.0510e-01,\n",
      "            1.0476e+00,  1.1362e-01, -4.5865e-01,  6.7152e-01, -5.1939e-01,\n",
      "            3.1871e-01,  1.8260e+00, -6.4009e-01, -2.2046e-01, -5.7007e-01,\n",
      "           -1.1305e+00, -5.8224e-01, -5.4640e-01],\n",
      "          [ 1.5804e+00,  1.7115e-01, -4.4693e-01, -3.5086e-01, -1.9599e-01,\n",
      "           -1.0048e+00,  4.0436e-01,  1.2834e+00, -7.4450e-01,  8.5554e-01,\n",
      "           -2.3725e-01, -1.2527e+00, -8.5556e-01, -8.1931e-01,  9.8954e-02,\n",
      "           -1.4131e-01, -4.2582e-01,  5.1764e-01,  9.8712e-01, -1.1158e+00,\n",
      "            7.1524e-01, -5.6096e-01, -1.1044e+00,  9.6357e-01, -3.7047e-01,\n",
      "           -1.4859e+00, -1.3135e+00,  6.6904e-01],\n",
      "          [ 3.1944e-01, -2.1666e-01,  1.0192e+00, -1.6555e-02, -9.5087e-01,\n",
      "            4.9711e-01, -3.8562e-01, -3.1381e-01,  1.6060e+00, -4.5033e-01,\n",
      "            1.6736e+00,  1.3579e+00, -1.1427e+00, -1.5605e+00,  3.3545e-01,\n",
      "            1.4863e+00,  1.1425e+00,  8.5045e-01,  1.4512e+00,  2.9411e-01,\n",
      "            1.0373e+00, -5.4210e-01, -3.0307e-01,  1.7392e-01,  1.0504e-01,\n",
      "           -4.6779e-01, -1.0724e+00, -2.6393e-01],\n",
      "          [ 6.6424e-01,  2.6769e-01,  8.7250e-01,  1.7459e+00,  1.3106e+00,\n",
      "            1.0720e+00,  7.1616e-01,  7.0954e-01, -1.3110e+00,  2.1982e-01,\n",
      "           -1.0399e+00, -1.5003e-01,  1.3549e+00, -1.6099e-01, -1.0396e+00,\n",
      "            8.0477e-01,  5.5198e-02, -2.5759e-01, -7.7363e-01,  5.9318e-01,\n",
      "           -2.6467e-01,  1.3831e+00, -2.3443e-01,  1.0469e+00,  1.8910e+00,\n",
      "            2.2299e+00,  3.6686e-01,  1.0498e+00],\n",
      "          [ 1.8960e+00, -7.9168e-01, -3.3292e-01, -4.2942e-01,  2.4827e-01,\n",
      "            4.7794e-01,  1.9849e+00, -1.6677e-01, -8.9599e-01,  9.2829e-01,\n",
      "            4.3895e-01, -9.8646e-01,  1.5366e+00,  8.5199e-01, -2.2319e+00,\n",
      "           -8.5748e-01, -3.4686e-02, -2.6823e-01,  1.5596e+00, -3.6882e-01,\n",
      "           -1.1485e+00, -1.3763e-01, -4.4680e-02,  1.8908e+00,  1.1010e+00,\n",
      "            1.3960e+00,  9.4335e-01,  1.1489e+00],\n",
      "          [-1.4447e+00, -1.1765e+00, -1.5613e-01, -7.9001e-01, -8.3569e-01,\n",
      "            2.0212e-01,  1.6710e-02,  3.4739e-02, -1.0727e-01, -1.1541e+00,\n",
      "            1.1938e+00, -1.1289e+00, -6.5880e-01,  4.1623e-01,  2.0201e+00,\n",
      "            1.3133e+00, -8.7278e-01,  1.3124e-01, -1.5460e+00, -6.4147e-01,\n",
      "           -1.2160e-03, -3.0408e-01,  1.7561e+00,  4.1720e-01,  7.2237e-01,\n",
      "            9.8742e-01,  1.1411e-01, -1.0509e+00],\n",
      "          [ 7.5128e-02, -9.4817e-01, -7.8493e-03,  4.6607e-01,  3.4167e-01,\n",
      "            1.8095e-01,  1.3260e+00, -6.6577e-01, -7.1155e-01, -1.0876e+00,\n",
      "           -4.2038e-01,  1.5647e+00,  1.1679e+00, -2.2799e+00, -1.8591e-01,\n",
      "            2.5868e-01,  2.7640e-01,  1.3280e+00, -1.0830e+00, -1.2554e+00,\n",
      "            1.0190e+00,  6.0311e-01,  1.1199e-01,  1.8629e+00,  5.4102e-01,\n",
      "           -1.0166e+00, -1.0069e+00, -1.0574e+00],\n",
      "          [ 1.6561e-01, -1.8065e-01, -7.1738e-01, -1.2356e-01,  5.6056e-01,\n",
      "            5.2774e-01,  1.0143e+00,  1.0340e+00,  7.2399e-01, -8.8559e-01,\n",
      "            3.8217e-01, -1.3927e+00, -4.2494e-01, -1.3174e-01, -6.8621e-01,\n",
      "            8.4226e-02,  5.3362e-01,  1.4678e+00, -3.6542e-01, -7.5513e-01,\n",
      "           -7.4102e-01, -8.9991e-01,  3.0877e-01,  3.3193e-01,  1.0623e+00,\n",
      "            6.9369e-01,  1.5672e+00, -3.6660e-01],\n",
      "          [-3.6661e-01, -8.8894e-01, -3.4332e-01,  4.9336e-03, -5.3501e-01,\n",
      "           -1.3714e+00,  1.3496e-01, -3.5003e-01,  1.8906e+00,  1.0323e+00,\n",
      "            1.0928e+00,  1.2119e+00, -2.7213e-01, -2.0161e+00,  9.5815e-01,\n",
      "           -2.9238e-01, -1.1244e+00,  1.5135e+00,  1.5190e+00,  2.8372e+00,\n",
      "            3.1701e-01,  4.4509e-01, -6.8181e-01,  1.3769e+00,  1.0920e+00,\n",
      "            5.4093e-01,  1.0883e+00, -2.1531e-01],\n",
      "          [-8.7289e-01, -1.2156e+00, -5.2063e-01, -9.4790e-02,  3.7735e+00,\n",
      "            1.1029e+00, -4.5050e-01, -5.9993e-01, -3.7554e-01, -4.9376e-01,\n",
      "            6.9996e-02, -2.1056e+00, -3.5783e-01,  1.9507e-01, -9.5803e-01,\n",
      "           -1.2667e+00,  1.8297e+00, -1.9189e+00, -2.3599e+00, -4.1953e-01,\n",
      "           -2.2557e-01, -7.0867e-01,  4.2379e-02,  2.2328e+00,  3.5183e-01,\n",
      "            1.3682e+00, -2.7002e-01,  1.2859e+00],\n",
      "          [-2.7737e-01, -4.7428e-01, -3.6045e-01,  5.5898e-01, -6.6888e-01,\n",
      "            2.7592e-01,  1.1698e-01,  2.4180e-02,  2.7760e-01,  4.7943e-01,\n",
      "           -8.5962e-01,  2.2702e-01,  1.1113e+00,  4.8664e-01,  1.5855e+00,\n",
      "           -3.0467e-02,  2.8129e-01, -5.4615e-01, -2.7764e-01, -1.0079e+00,\n",
      "           -1.1230e+00,  6.0764e-01,  2.8573e-01, -9.4756e-01,  2.0547e-01,\n",
      "           -2.2971e+00, -1.7112e-01,  1.7350e-01],\n",
      "          [ 4.0630e-02,  9.2242e-01, -7.8705e-01,  1.2490e-01,  8.9682e-01,\n",
      "           -1.4226e+00, -5.0484e-01, -1.0843e+00, -7.0726e-02, -1.3186e+00,\n",
      "            2.2008e+00,  1.0372e+00, -1.1639e-01, -5.0868e-01, -2.6015e+00,\n",
      "           -7.8161e-01,  8.9493e-01, -1.3907e+00,  5.9629e-01, -1.0412e+00,\n",
      "           -1.8265e+00,  6.7911e-01,  6.0800e-01, -3.7867e-01,  4.3626e-01,\n",
      "           -1.1156e+00, -9.0077e-01, -4.1446e-01],\n",
      "          [ 9.4613e-01,  1.0001e+00,  3.0821e-01, -5.5455e-01,  1.7069e-01,\n",
      "            1.4003e-01, -6.5237e-01,  1.0692e+00, -1.8281e+00, -3.1509e-01,\n",
      "           -1.5198e-01,  7.6028e-02, -6.2844e-01, -8.4965e-01, -8.6376e-01,\n",
      "           -6.1304e-01, -5.5972e-01,  3.7245e-01, -4.6775e-01, -9.4073e-02,\n",
      "            1.2349e-02,  8.7171e-01, -7.3581e-01, -3.5745e-01, -1.2693e+00,\n",
      "            7.9593e-01, -6.7509e-02, -1.9753e+00],\n",
      "          [-6.8933e-01, -2.8031e-01,  6.9686e-01, -1.1990e+00, -9.4067e-01,\n",
      "           -4.5644e-01, -7.5756e-01, -2.2911e+00,  3.4934e-01,  7.9019e-01,\n",
      "            2.4164e-01,  6.5989e-01, -2.1354e-03, -9.0553e-01, -8.3608e-01,\n",
      "            3.7487e-02, -3.1567e-01,  1.7497e+00,  7.0220e-01,  3.9758e-01,\n",
      "           -2.3288e+00, -1.2564e-01, -1.0390e-01,  9.0734e-01, -1.3462e-01,\n",
      "            7.5310e-01, -1.2245e-01, -3.9300e-01],\n",
      "          [ 1.0690e+00, -8.6830e-01, -7.1027e-01, -7.8471e-03, -8.8867e-01,\n",
      "            4.5255e-01,  6.9071e-01, -2.5379e-01, -2.4745e+00, -3.4989e-01,\n",
      "            8.3212e-01,  2.9243e-01,  6.7092e-01, -7.3335e-01, -1.0518e+00,\n",
      "           -4.8783e-01,  1.4677e+00,  1.1032e+00,  3.0419e-03, -9.0952e-03,\n",
      "           -1.8047e+00,  1.8552e+00,  4.5649e-01, -3.3709e-01,  3.7020e-01,\n",
      "            4.3095e-01,  1.2611e+00, -2.3543e-01],\n",
      "          [-7.8840e-01,  6.2355e-01,  1.2289e-01,  1.0266e-01,  1.3317e+00,\n",
      "           -3.9917e-01, -5.2608e-01,  6.6419e-01, -2.1595e+00,  9.6385e-01,\n",
      "           -7.6042e-01,  1.4692e+00, -5.5653e-01, -2.3897e-01,  9.1552e-01,\n",
      "           -2.2511e-01, -1.0386e+00, -1.0100e-01, -6.0755e-01, -9.8109e-01,\n",
      "            8.6832e-01,  9.3771e-02, -1.6817e+00, -1.3581e+00, -1.6163e-01,\n",
      "            5.5436e-01, -6.9516e-01, -1.4863e-01],\n",
      "          [ 1.3751e+00,  3.5881e-01, -3.1100e-01,  2.1350e-01,  6.2716e-02,\n",
      "            6.9821e-01,  5.7717e-01, -1.7143e+00,  1.2481e-01, -2.4932e-01,\n",
      "           -9.4775e-01,  1.1180e+00,  2.3787e+00, -2.0035e+00,  8.1345e-01,\n",
      "            2.8052e-01,  1.4170e+00,  2.4370e-01,  9.3594e-01, -1.0925e+00,\n",
      "           -9.8086e-01, -6.9244e-01, -3.9455e-01,  1.7696e+00,  1.8767e+00,\n",
      "           -3.7981e-01, -3.6431e-01, -8.0299e-01],\n",
      "          [-2.6633e-01, -5.2227e-01,  1.7923e+00,  2.3330e-01, -1.7562e-01,\n",
      "           -3.3600e-01,  1.4397e+00, -9.0728e-01,  4.1308e-01,  1.3985e+00,\n",
      "           -5.8525e-03, -1.1391e+00, -1.4386e+00,  1.1486e-01,  1.0714e+00,\n",
      "            1.3675e+00, -4.2826e-01, -1.2938e+00, -8.9450e-01, -2.6430e-01,\n",
      "           -5.3720e-01,  5.2362e-01,  1.1938e+00,  4.1397e-01,  7.8534e-01,\n",
      "            1.5644e+00,  7.6794e-01, -1.9232e-01],\n",
      "          [ 7.4971e-01, -6.6418e-01,  6.5507e-01,  6.8693e-01, -2.8922e-01,\n",
      "           -6.0661e-01, -5.3359e-01,  7.3378e-01,  7.5604e-01, -1.2468e+00,\n",
      "            5.7819e-01, -6.8628e-01,  4.2360e-01,  1.7995e+00, -1.6598e+00,\n",
      "           -4.6070e-01,  3.7506e-01, -5.6624e-01, -8.5151e-01,  2.8980e-01,\n",
      "           -2.1749e-01,  5.0346e-01, -9.6701e-01, -2.2756e+00,  9.4420e-01,\n",
      "            6.5406e-01,  2.2948e+00, -8.4716e-01],\n",
      "          [ 1.7444e+00, -1.1550e+00,  4.9205e-01,  1.2346e+00,  1.5830e+00,\n",
      "            1.4654e+00,  4.7860e-01, -5.3229e-01, -5.9584e-01, -1.6457e+00,\n",
      "            2.8973e-01,  1.0991e-01, -2.4655e-01, -1.1140e-01,  8.6125e-01,\n",
      "           -9.7084e-01,  3.0406e+00, -1.5798e+00, -2.0642e-01, -7.6871e-01,\n",
      "           -1.0179e+00, -6.2446e-01, -3.0621e-01,  8.8220e-01, -9.2910e-01,\n",
      "            1.1586e+00,  1.4870e-02, -1.9008e+00]]]])]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 10 / 10 (100.0%)\nGreatest absolute difference: 131.59054565429688 at index (9,) (up to 0.001 allowed)\nGreatest relative difference: 1.2952085733413696 at index (2,) (up to 0.001 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(torch_outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(onnxruntime_outputs)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m torch_output, onnxruntime_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(torch_outputs, onnxruntime_outputs):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnxruntime_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch and ONNX Runtime output matched!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(onnxruntime_outputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\cours\\IA\\.venv\\Lib\\site-packages\\torch\\testing\\_comparison.py:1530\u001b[0m, in \u001b[0;36massert_close\u001b[1;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[0;32m   1508\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[0;32m   1509\u001b[0m     actual,\n\u001b[0;32m   1510\u001b[0m     expected,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1525\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[0;32m   1526\u001b[0m )\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[0;32m   1529\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[1;32m-> 1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 10 / 10 (100.0%)\nGreatest absolute difference: 131.59054565429688 at index (9,) (up to 0.001 allowed)\nGreatest relative difference: 1.2952085733413696 at index (2,) (up to 0.001 allowed)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
